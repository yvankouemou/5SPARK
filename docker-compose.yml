version: '3'

services:
  zookeeper:
    image: wurstmeister/zookeeper:3.4.6
    ports:
      - "2181:2181"
    volumes:
      - zookeeper_data:/data
    networks:
      - data_network

  kafka:
    image: wurstmeister/kafka:latest
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9093,OUTSIDE://localhost:9092
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: "message_kafka:1:1"  # Création automatique du topic message_kafka avec 1 partition et 1 réplique
    volumes:
      - kafka_data:/kafka
    networks:
      - data_network
    depends_on:
      - zookeeper
    healthcheck:
      test: ["CMD", "kafka-topics.sh", "--list", "--zookeeper", "zookeeper:2181"]
      interval: 30s
      timeout: 10s
      retries: 3

  mastodon_kafka_stream:          #mise en place du Dockerfile
    build:
      context: .  # Contexte de construction du conteneur
      dockerfile: Dockerfile
    depends_on:
      - kafka

  spark:
    image: bitnami/spark:latest
    ports:
      - "8080:8080"  # Port pour l'interface web de Spark
      - "7077:7077"  # Port pour le maître Spark
    environment:
      SPARK_MASTER_URL: "spark://spark:7077"
      SPARK_JARS_PACKAGES: "org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.kafka:kafka-clients:2.7.0"
      SPARK_MASTER_HOST: spark
      SPARK_MASTER_PORT: 7077
      SPARK_LOCAL_IP: spark  # Important pour que Spark puisse résoudre le nom du conteneur
    volumes:
      - ./jars/spark-sql-kafka-0-10_2.12-3.2.0.jar:/opt/bitnami/spark/jars/spark-sql-kafka-0-10_2.12-3.2.0.jar
      - ./jars/kafka-clients-2.7.0.jar:/opt/bitnami/spark/jars/kafka-clients-2.7.0.jar

    networks:
      - data_network
    depends_on:
      - kafka
    deploy:
      resources:
        limits:
          cpus: '0.75'
          memory: 1G

  jupyter:
    image: jupyter/pyspark-notebook:latest
    ports:
      - "8888:8888"  # Port pour Jupyter Notebook
    environment:
      JUPYTER_ENABLE_LAB: "yes"  # Active JupyterLab
      SPARK_EXTRA_JAVA_OPTS: "-Djava.library.path=/usr/lib/hadoop/lib/native"
    volumes:
      - ./notebooks:/home/jovyan/work  # Dossier local pour les notebooks
    networks:
      - data_network
    depends_on:
      - spark

  postgres:
    image: postgres:13
    environment:
      POSTGRES_DB: mastodon_data  # Nom de la base de données
      POSTGRES_USER: user  # Nom de l'utilisateur
      POSTGRES_PASSWORD: password   # Mot de passe de l'utilisateur
    ports:
      - "5432:5432"  # Port pour PostgreSQL
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - data_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user -d mastodon_data"]
      interval: 10s
      timeout: 5s
      retries: 5

networks:
  data_network:
    driver: bridge

volumes:
  zookeeper_data:
  kafka_data:
  postgres_data:
